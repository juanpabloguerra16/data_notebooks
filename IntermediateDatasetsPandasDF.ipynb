{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def mountFileSystem(containerName, storageAccountName):\n",
    "  configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "       \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "       \"fs.azure.account.oauth2.client.id\": \"<Service Principal Application ID>\",\n",
    "       \"fs.azure.account.oauth2.client.secret\": \"<Service Principal Application Secret (or password)>\",\n",
    "       \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/<Service Principal Tenant ID>/oauth2/token\",\n",
    "       \"fs.azure.createRemoteFileSystemDuringInitialization\": \"true\"}\n",
    "  \n",
    "  mountPoint = \"/mnt/adls/\" + containerName\n",
    "  \n",
    "  try:\n",
    "    dbutils.fs.mount(\n",
    "      source = \"abfss://\" + containerName + \"@\" + storageAccountName + \".dfs.core.windows.net\",\n",
    "      mount_point = mountPoint,\n",
    "      extra_configs = configs\n",
    "    )\n",
    "    print(mountPoint + \" mounted successfully\")\n",
    "  except:\n",
    "    print(\"The mount \" + mountPoint + \" already exists.\")\n",
    "\n",
    "  return mountPoint\n",
    "\n",
    "def saveAsParquet(dataFrame, filePath):\n",
    "  df = sqlContext.createDataFrame(dataFrame)\n",
    "\n",
    "  df.write.parquet(filePath, mode='overwrite')\n",
    "\n",
    "  print(filePath + \" saved successfully\")\n",
    "\n",
    "mountPoint = mountFileSystem(\"southridge\", \"<storage account name>\")\n",
    "print(mountPoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Southridge Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_sales_customers_parquet = mountPoint + \"/raw/cloudsales/Customers.parquet\"\n",
    "sr_streaming_customers_parquet = mountPoint + \"/raw/cloudstreaming/Customers.parquet\"\n",
    "\n",
    "sr_sales_customers = sqlContext.read.parquet(sr_sales_customers_parquet)\n",
    "sr_streaming_customers = sqlContext.read.parquet(sr_streaming_customers_parquet)\n",
    "\n",
    "sr_sales_customers = sr_sales_customers.toPandas()\n",
    "sr_streaming_customers = sr_streaming_customers.toPandas()\n",
    "\n",
    "sr_customers_frame = [sr_sales_customers, sr_streaming_customers]\n",
    "sr_customers = pd.concat(sr_customers_frame)\n",
    "\n",
    "sr_customers['CreatedDate'] = pd.to_datetime(sr_customers['CreatedDate'], errors='coerce')\n",
    "sr_customers['UpdatedDate'] = pd.to_datetime(sr_customers['UpdatedDate'], errors='coerce')\n",
    "sr_customers.PhoneNumber = sr_customers.PhoneNumber.astype(str)\n",
    "\n",
    "display(sr_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading VanArsdel Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_customers_filePath = mountPoint + \"/raw/vanarsdel/Customers.json\"\n",
    "\n",
    "va_customers_raw = spark.read.option(\"multiLine\", \"true\").options(header='true', inferschema='true').json(va_customers_filePath)\n",
    "va_customers_pd = va_customers_raw.toPandas()\n",
    "\n",
    "va_customers = va_customers_pd[['CustomerID', 'LastName', 'FirstName', 'PhoneNumber', 'CreatedDate', 'UpdatedDate']]\n",
    "\n",
    "va_customers['CreatedDate'] = pd.to_datetime(va_customers_pd['CreatedDate'], errors='coerce')\n",
    "va_customers['UpdatedDate'] = pd.to_datetime(va_customers_pd['UpdatedDate'], errors='coerce')\n",
    "\n",
    "va_customers.PhoneNumber = va_customers_pd.PhoneNumber.astype(str)\n",
    "\n",
    "display(va_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading FourthCoffee Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_customers_filePath = \"/dbfs\" + mountPoint + \"/raw/fourthcoffee/Customers.csv\"\n",
    "\n",
    "fc_customers_pd = pd.read_csv(fc_customers_filePath)\n",
    "\n",
    "fc_customers = fc_customers_pd[['CustomerID', 'LastName', 'FirstName', 'PhoneNumber', 'CreatedDate', 'UpdatedDate']]\n",
    "\n",
    "fc_customers['CreatedDate'] = pd.to_datetime(fc_customers_pd['CreatedDate'], errors='coerce')\n",
    "fc_customers['UpdatedDate'] = pd.to_datetime(fc_customers_pd['UpdatedDate'], errors='coerce')\n",
    "fc_customers.PhoneNumber = fc_customers_pd.PhoneNumber.astype(str)\n",
    "\n",
    "display(fc_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Southridge Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_sales_addresses_parquet = mountPoint + \"/raw/cloudsales/Addresses.parquet\"\n",
    "sr_streaming_addresses_parquet = mountPoint + \"/raw/cloudstreaming/Addresses.parquet\"\n",
    "\n",
    "sr_sales_addresses = sqlContext.read.parquet(sr_sales_addresses_parquet)\n",
    "sr_streaming_addresses = sqlContext.read.parquet(sr_streaming_addresses_parquet)\n",
    "\n",
    "sr_sales_addresses = sr_sales_addresses.toPandas()\n",
    "sr_streaming_addresses = sr_streaming_addresses.toPandas()\n",
    "\n",
    "sr_addresses_frame = [sr_sales_addresses, sr_streaming_addresses]\n",
    "sr_addresses = pd.concat(sr_addresses_frame)\n",
    "\n",
    "sr_addresses['CreatedDate'] = pd.to_datetime(sr_addresses['CreatedDate'], errors='coerce')\n",
    "sr_addresses['UpdatedDate'] = pd.to_datetime(sr_addresses['UpdatedDate'], errors='coerce')\n",
    "\n",
    "sr_addresses.AddressLine2 = sr_addresses.AddressLine2.astype(str)\n",
    "sr_addresses.ZipCode = sr_addresses.ZipCode.astype(str)\n",
    "\n",
    "display(sr_addresses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading VanArsdel Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_customers_filePath = mountPoint + \"/raw/vanarsdel/Customers.json\"\n",
    "\n",
    "va_customers_raw = spark.read.option(\"multiLine\", \"true\").options(header='true', inferschema='true').json(va_customers_filePath)\n",
    "va_customers_pd = va_customers_raw.toPandas()\n",
    "\n",
    "va_addresses = va_customers_pd[['CustomerID', 'AddressLine1', 'AddressLine2', 'City', 'State', 'ZipCode', 'CreatedDate', 'UpdatedDate']]\n",
    "\n",
    "va_addresses['CreatedDate'] = pd.to_datetime(va_addresses['CreatedDate'], errors='coerce')\n",
    "va_addresses['UpdatedDate'] = pd.to_datetime(va_addresses['UpdatedDate'], errors='coerce')\n",
    "\n",
    "va_addresses.AddressLine2 = va_addresses.AddressLine2.astype(str)\n",
    "va_addresses.ZipCode = va_addresses.ZipCode.astype(str)\n",
    "\n",
    "va_addresses.insert(0, 'AddressID', 'None')\n",
    "\n",
    "display(va_addresses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading FourthCoffee Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_customers_filePath = \"/dbfs\" + mountPoint + \"/raw/fourthcoffee/Customers.csv\"\n",
    "\n",
    "fc_customers_pd = pd.read_csv(fc_customers_filePath)\n",
    "\n",
    "fc_addresses = fc_customers_pd[['CustomerID', 'AddressLine1', 'AddressLine2', 'City', 'State', 'ZipCode', 'CreatedDate', 'UpdatedDate']]\n",
    "\n",
    "fc_addresses['CreatedDate'] = pd.to_datetime(fc_addresses['CreatedDate'], errors='coerce')\n",
    "fc_addresses['UpdatedDate'] = pd.to_datetime(fc_addresses['UpdatedDate'], errors='coerce')\n",
    "fc_addresses.AddressLine2 = fc_addresses.AddressLine2.astype(str)\n",
    "fc_addresses.ZipCode = fc_addresses.ZipCode.astype(str)\n",
    "\n",
    "fc_addresses.loc[fc_addresses.AddressLine2 == 'nan', 'AddressLine2'] = 'None'\n",
    "\n",
    "fc_addresses.insert(0, 'AddressID', 'None')\n",
    "\n",
    "display(fc_addresses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Southridge Movies and Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_movies_filepath = mountPoint + \"/raw/moviescatalog/movies.json\"\n",
    "\n",
    "sr_movies_raw = spark.read.option(\"multiLine\", \"true\").options(header='true', inferschema='true').json(sr_movies_filepath)\n",
    "sr_movies_pd = sr_movies_raw.toPandas()\n",
    "\n",
    "sr_movies_pd = sr_movies_pd[['actors', 'availabilityDate', 'genre', 'id', 'rating', 'releaseYear', 'runtime', 'streamingAvailabilityDate', 'tier', 'title']]\n",
    "\n",
    "movieactors = sr_movies_pd[['id', 'actors']]\n",
    "movies = sr_movies_pd[['id', 'title', 'genre', 'availabilityDate', 'rating', 'releaseYear', 'runtime', 'streamingAvailabilityDate', 'tier']]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "actorslist = movieactors.actors.values.tolist()\n",
    "actorcountbymovie = [len(r) for r in actorslist]\n",
    "explodedmovieids = np.repeat(movieactors.id, actorcountbymovie)\n",
    "\n",
    "movieactors = pd.DataFrame(np.column_stack((explodedmovieids, np.concatenate(actorslist))), columns=movieactors.columns)\n",
    "\n",
    "sr_all_moviesactorsactors = pd.merge(movies, movieactors, on='id')\n",
    "\n",
    "sr_all_moviesactorsactors = sr_all_moviesactorsactors.rename(index=str, columns={'id': 'MovieID', 'title': 'MovieTitle', 'genre': 'Genre', 'availabilityDate': 'AvailabilityDate', 'rating': 'Rating', 'releaseYear': 'ReleaseYear', 'runtime': 'RuntimeMin', 'streamingAvailabilityDate': 'StreamingAvailabilityDate', 'tier': 'Tier', 'actors': 'ActorName'})\n",
    "\n",
    "sr_all_moviesactorsactors['ActorID'] = 'None'\n",
    "sr_all_moviesactorsactors['MovieActorID'] = 'None'\n",
    "sr_all_moviesactorsactors['ActorGender'] = 'None'\n",
    "sr_all_moviesactorsactors['ReleaseDate'] = 'None'\n",
    "\n",
    "sr_all_moviesactorsactors.ReleaseYear = sr_all_moviesactorsactors.ReleaseYear.astype(str)\n",
    "sr_all_moviesactorsactors.Tier = sr_all_moviesactorsactors.Tier.astype(str)\n",
    "sr_all_moviesactorsactors.RuntimeMin = sr_all_moviesactorsactors.RuntimeMin.astype(str)\n",
    "\n",
    "sr_all_moviesactorsactors = sr_all_moviesactorsactors[['MovieID', 'MovieTitle', 'Genre', 'ReleaseDate', 'AvailabilityDate', 'StreamingAvailabilityDate', 'ReleaseYear', 'Tier', 'Rating', 'RuntimeMin', 'MovieActorID', 'ActorID', 'ActorName', 'ActorGender']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading VanArsdel Movies and Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_movies_filepath = mountPoint + \"/raw/vanarsdel/Movies.json\"\n",
    "va_actors_filepath = mountPoint + \"/raw/vanarsdel/Actors.json\"\n",
    "va_movieactors_filepath = mountPoint + \"/raw/vanarsdel/MovieActors.json\"\n",
    "\n",
    "va_movies_raw = spark.read.option(\"multiLine\", \"true\").options(header='true', inferschema='true').json(va_movies_filepath)\n",
    "va_actors_raw = spark.read.option(\"multiLine\", \"true\").options(header='true', inferschema='true').json(va_actors_filepath)\n",
    "va_movieactors_raw = spark.read.option(\"multiLine\", \"true\").options(header='true', inferschema='true').json(va_movieactors_filepath)\n",
    "\n",
    "va_movies_pd = va_movies_raw.toPandas()\n",
    "va_actors_pd = va_actors_raw.toPandas()\n",
    "va_movieactors_pd = va_movieactors_raw.toPandas()\n",
    "\n",
    "va_all_movies = pd.merge(va_movieactors_pd, va_movies_pd, on='MovieID')\n",
    "va_all_movies = pd.merge(va_all_movies, va_actors_pd, on='ActorID')\n",
    "\n",
    "va_all_movies = va_all_movies.rename(index=str, columns={'Category': 'Genre', 'RunTimeMin': 'RuntimeMin', 'Gender': 'ActorGender'})\n",
    "\n",
    "va_all_movies['AvailabilityDate'] = 'None'\n",
    "va_all_movies['StreamingAvailabilityDate'] = 'None'\n",
    "va_all_movies['ReleaseYear'] = 'None'\n",
    "va_all_movies['Tier'] = 'None'\n",
    "\n",
    "va_all_movies.RuntimeMin = va_all_movies.RuntimeMin.astype(str)\n",
    "\n",
    "va_all_movies = va_all_movies[['MovieID', 'MovieTitle', 'Genre', 'ReleaseDate', 'AvailabilityDate', 'StreamingAvailabilityDate', 'ReleaseYear', 'Tier', 'Rating', 'RuntimeMin', 'MovieActorID', 'ActorID', 'ActorName', 'ActorGender']]\n",
    "\n",
    "va_all_movies.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading FourthCoffee Movies and Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_movies_filepath = \"/dbfs/\" + mountPoint + \"/raw/fourthcoffee/Movies.csv\"\n",
    "fc_actors_filepath = \"/dbfs/\" + mountPoint + \"/raw/fourthcoffee/Actors.csv\"\n",
    "fc_movieactors_filepath = \"/dbfs/\" + mountPoint + \"/raw/fourthcoffee/MovieActors.csv\"\n",
    "\n",
    "fc_movies_pd = pd.read_csv(fc_movies_filepath)\n",
    "fc_actors_pd = pd.read_csv(fc_actors_filepath)\n",
    "fc_movieactors_pd = pd.read_csv(fc_movieactors_filepath)\n",
    "\n",
    "fc_all_moviesactors = pd.merge(fc_movieactors_pd, fc_movies_pd, on='MovieID')\n",
    "fc_all_moviesactors = pd.merge(fc_all_moviesactors, fc_actors_pd, on='ActorID')\n",
    "\n",
    "fc_all_moviesactors = fc_all_moviesactors.rename(index=str, columns={'Category': 'Genre', 'RunTimeMin': 'RuntimeMin', 'Gender': 'ActorGender'})\n",
    "\n",
    "fc_all_moviesactors['AvailabilityDate'] = 'None'\n",
    "fc_all_moviesactors['StreamingAvailabilityDate'] = 'None'\n",
    "fc_all_moviesactors['ReleaseYear'] = 'None'\n",
    "fc_all_moviesactors['Tier'] = 'None'\n",
    "\n",
    "fc_all_moviesactors.RuntimeMin = fc_all_moviesactors.RuntimeMin.astype(str)\n",
    "\n",
    "fc_all_moviesactors = fc_all_moviesactors[['MovieID', 'MovieTitle', 'Genre', 'ReleaseDate', 'AvailabilityDate', 'StreamingAvailabilityDate', 'ReleaseYear', 'Tier', 'Rating', 'RuntimeMin', 'MovieActorID', 'ActorID', 'ActorName', 'ActorGender']]\n",
    "\n",
    "display(fc_all_moviesactors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing all Customers together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_customers['SourceSystem'] = 'southridge'\n",
    "va_customers['SourceSystem'] = 'vanarsdel'\n",
    "fc_customers['SourceSystem'] = 'fourthcoffee'\n",
    "customers_frame = [sr_customers, va_customers, fc_customers]\n",
    "\n",
    "all_customers = pd.concat(customers_frame)\n",
    "\n",
    "display(all_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing all Addresses together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_addresses['SourceSystem'] = 'southridge'\n",
    "va_addresses['SourceSystem'] = 'vanarsdel'\n",
    "fc_addresses['SourceSystem'] = 'fourthcoffee'\n",
    "addresses_frame = [sr_addresses, va_addresses, fc_addresses]\n",
    "\n",
    "all_addresses = pd.concat(addresses_frame)\n",
    "\n",
    "display(all_addresses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing all Movies and Actors together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_all_moviesactors['SourceSystem'] = 'southridge'\n",
    "va_all_moviesactors['SourceSystem'] = 'vanarsdel'\n",
    "fc_all_moviesactors['SourceSystem'] = 'fourthcoffee'\n",
    "\n",
    "moviesactors_frame = [sr_all_moviesactors, va_all_moviesactors, fc_all_moviesactors]\n",
    "\n",
    "all_moviesactors = pd.concat(moviesactors_frame)\n",
    "\n",
    "display(all_moviesactors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the data as parquet to the data lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = mountPoint + '/parquet/<Business Object name>.parquet'\n",
    "\n",
    "saveAsParquet(all_<business object name>, parquet_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
